{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proyecto - Statistical Learning I**\n",
    "\n",
    "### Desarrollo del Proyecto\n",
    "\n",
    "#### Paquetes a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from joblib import dump\n",
    "import os\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\eezg_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.\"):\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.compat.v1.disable_v2_behavior()\n",
    "    tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "      <th>passenger_survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>887</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Middle</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>888</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>889</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>890</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>891</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId                                               Name   Age  \\\n",
       "0              1                            Braund, Mr. Owen Harris  22.0   \n",
       "1              2  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0   \n",
       "2              3                             Heikkinen, Miss. Laina  26.0   \n",
       "3              4       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0   \n",
       "4              5                           Allen, Mr. William Henry  35.0   \n",
       "..           ...                                                ...   ...   \n",
       "886          887                              Montvila, Rev. Juozas  27.0   \n",
       "887          888                       Graham, Miss. Margaret Edith  19.0   \n",
       "888          889           Johnston, Miss. Catherine Helen \"Carrie\"   NaN   \n",
       "889          890                              Behr, Mr. Karl Howell  26.0   \n",
       "890          891                                Dooley, Mr. Patrick  32.0   \n",
       "\n",
       "     SibSp  Parch            Ticket     Fare Cabin Embarked passenger_class  \\\n",
       "0        1      0         A/5 21171   7.2500   NaN        S           Lower   \n",
       "1        1      0          PC 17599  71.2833   C85        C           Upper   \n",
       "2        0      0  STON/O2. 3101282   7.9250   NaN        S           Lower   \n",
       "3        1      0            113803  53.1000  C123        S           Upper   \n",
       "4        0      0            373450   8.0500   NaN        S           Lower   \n",
       "..     ...    ...               ...      ...   ...      ...             ...   \n",
       "886      0      0            211536  13.0000   NaN        S          Middle   \n",
       "887      0      0            112053  30.0000   B42        S           Upper   \n",
       "888      1      2        W./C. 6607  23.4500   NaN        S           Lower   \n",
       "889      0      0            111369  30.0000  C148        C           Upper   \n",
       "890      0      0            370376   7.7500   NaN        Q           Lower   \n",
       "\n",
       "    passenger_sex passenger_survived  \n",
       "0               M                  N  \n",
       "1               F                  Y  \n",
       "2               F                  Y  \n",
       "3               F                  Y  \n",
       "4               M                  N  \n",
       "..            ...                ...  \n",
       "886             M                  N  \n",
       "887             F                  Y  \n",
       "888             F                  N  \n",
       "889             M                  Y  \n",
       "890             M                  N  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_titanic = pd.read_csv('data_titanic_proyecto.csv')\n",
    "data_titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de datos\n",
    "\n",
    "Convertir valores NaN a cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_titanic = data_titanic.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de Variables\n",
    "\n",
    "Posibles variables predictoras\n",
    "- Age\n",
    "- Fare\n",
    "- passenger_class\n",
    "- passenger_sex\n",
    "\n",
    "Variable a predecir\n",
    "\n",
    "- passenger_survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversión de variables para determinar nivel de correlación\n",
    "\n",
    "A continuación se convierten aquellas variables categóricas a un factor númerico, esta conversión se hace únicamente para determinar el nivel de correlación de las variables independientes con la variable dependiente, pero no es una transformación de encoded para el proceso de entrenamiento.\n",
    "\n",
    "*La variable Age no requiere conversión ya que por defecto su valor es númerico*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_titanic['passenger_survived_codes'] = data_titanic['passenger_survived'].astype('category').cat.codes\n",
    "data_titanic['passenger_sex_codes'] = data_titanic['passenger_sex'].astype('category').cat.codes\n",
    "data_titanic['passenger_class_codes'] = data_titanic['passenger_class'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                         0.010539\n",
       "SibSp                      -0.035322\n",
       "Parch                       0.081629\n",
       "Fare                        0.257307\n",
       "passenger_survived_codes    1.000000\n",
       "passenger_sex_codes        -0.543351\n",
       "passenger_class_codes       0.338481\n",
       "Name: passenger_survived_codes, dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_titanic[data_titanic.columns[1:]].corr()['passenger_survived_codes'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depuración de features\n",
    "\n",
    "Se eliminan aquellas características que son identificadores, nombres o etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_titanic.drop(['passenger_survived_codes','PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked', 'passenger_class', 'passenger_sex', 'passenger_survived'], axis=1)\n",
    "Y=data_titanic['passenger_survived_codes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de mejores features\n",
    "\n",
    "Utilizando la libreria \"SelectKBest\" se determinan las 3 mejores características y sobre esas se trabajan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Fare', 'passenger_sex_codes', 'passenger_class_codes'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "best=SelectKBest(k=3)\n",
    "best.fit_transform(X, Y)\n",
    "selected = best.get_support(indices=True)\n",
    "print(X.columns[selected])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizarán las variables:\n",
    "\n",
    "- Fare\n",
    "- passenger_sex\n",
    "- passenger_class\n",
    "\n",
    "Ya que poseen un alto nivel de correlación y según la libreria \"SelectKBest\" las sugiere como mejores features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = X.columns[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la posibles features \"X\", se eliminan aquellas que no formaran parte del proceso de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['Age', 'SibSp', 'Parch'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### División de datos para entreno, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de funciones generales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para crear el archivo CSV que servirá de bitácora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitacora = {'string_config': [],\n",
    "            'accuracy': [],\n",
    "            'error': [],\n",
    "            'precision': [],\n",
    "            'recall': [],\n",
    "            'f1': []}\n",
    "\n",
    "df = pd.DataFrame(bitacora, columns= ['string_config', 'accuracy', 'error', 'precision', 'recall', 'f1'])\n",
    "df.to_csv('bitacora.csv', index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para reescribir el CSV de bitácora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_record(string_config, metrics):\n",
    "    bitacora = {'string_config': [string_config],\n",
    "                'accuracy': [metrics[0]],\n",
    "                'error': [metrics[1]],\n",
    "                'precision': [metrics[2]],\n",
    "                'recall': [metrics[3]],\n",
    "                'f1': [metrics[4]]}\n",
    "    bitacora = pd.DataFrame(bitacora, columns= ['string_config', 'accuracy', 'error', 'precision', 'recall', 'f1'])\n",
    "    bitacora.to_csv('bitacora.csv', mode='a', index = False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función que devuelve el string de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str_metrics(str_model, mtr):\n",
    "    return \"Model: \"+str_model+\" | Accuracy: \"+get_percent(mtr[0])+\" | Error: \"+get_percent(mtr[1])+\" | Precisión: \"+get_percent(mtr[2])+\" | Recall: \"+get_percent(mtr[3])+\" | F1: \"+get_percent(mtr[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función que da formato de porcentaje a una cantidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percent(number):\n",
    "    return str(round((number*100),2))+\"%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para generación de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_predict):\n",
    "    accuracy = accuracy_score(y_true, y_predict)\n",
    "    error = mean_squared_error(y_true, y_predict)\n",
    "    precision = precision_score(y_true, y_predict, average='weighted')\n",
    "    recall = recall_score(y_true, y_predict, average='weighted')\n",
    "    f1 = f1_score(y_true, y_predict, average=\"weighted\")\n",
    "    \n",
    "    return accuracy, error, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para guardar el modelo y registrar bitácora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_x_record(str_config, str_model, model, metrics, isSkLearn):\n",
    "    print(get_str_metrics(str_model, metrics))\n",
    "    set_record(str_config, metrics)\n",
    "    path = \"modelos/\"+str_config+\"_\"+datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if(isSkLearn):\n",
    "        dump(model, path+\".joblib\")\n",
    "    else:\n",
    "        if (str_model == \"Naive Bayes\"):\n",
    "            os.mkdir(path)\n",
    "            model[\"mean\"].to_csv(path+\"/mean.csv\", index = False, header=True)\n",
    "            model[\"stdev\"].to_csv(path+\"/stdev.csv\", index = False, header=True)        \n",
    "            pd.DataFrame(data=model[\"probabilities\"][0:2], columns=[\"probabilities\"]).to_csv(path+\"/probabilities.csv\", index = False, header=True)\n",
    "            pd.DataFrame(data=model[\"class\"][0:2], columns=[\"class\"]).to_csv(path+\"/class.csv\", index = False, header=True)\n",
    "        else:\n",
    "            pd.DataFrame(data=model[0], columns=[\"params\"]).to_csv(path+\".csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para determinar la moda de un conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moda(data):\n",
    "    repetitions = 0\n",
    "    moda = -1\n",
    "    for i in data:\n",
    "        n = data.count(i)\n",
    "        if n > repetitions:\n",
    "            repetitions = n\n",
    "    for i in data:\n",
    "        n = data.count(i)\n",
    "        if n == repetitions and moda == -1:\n",
    "            moda = i     \n",
    "    return moda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para obtener predición final en base a la moda de las predicciones individuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_prediction(prediction_joined):\n",
    "    moda_prediction = []\n",
    "    for predict in prediction_joined:\n",
    "        moda_predict = moda(list(predict))\n",
    "        moda_prediction.append(moda_predict)\n",
    "    return np.array(moda_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definción de Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_decision_tree(x_train, y_train, x_validate, y_validate):\n",
    "    \n",
    "    tree_model = tree.DecisionTreeClassifier()\n",
    "    tree_model = tree_model.fit(x_train, y_train)\n",
    "    y_predict = tree_model.predict(x_validate)\n",
    "    \n",
    "    return y_predict, tree_model, get_metrics(y_validate, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_svm(x_train, y_train, x_validate, y_validate):\n",
    "\n",
    "    svm_model = svm.SVC()\n",
    "    svm_model = svm_model.fit(x_train, y_train)\n",
    "    y_predict = svm_model.predict(x_validate)\n",
    "    \n",
    "    return y_predict, svm_model, get_metrics(y_validate, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(model, x_validate):\n",
    "    y_predict = []\n",
    "    for i in range(x_validate.shape[0]):\n",
    "        probability={}\n",
    "        for y_class in model[\"class\"]:\n",
    "            probability[y_class] = model[\"probabilities\"].iloc[y_class]\n",
    "            for index, _ in enumerate(x_validate.iloc[i]):\n",
    "                probability[y_class] *= norm.pdf(x_validate.iloc[i], model[\"mean\"].iloc[y_class, index], model[\"stdev\"].iloc[y_class, index])\n",
    "        y_predict.append(get_argmax(probability))\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_argmax(probability):\n",
    "    max_value = 0\n",
    "    argmax = -1\n",
    "    for (key, value) in probability.items():\n",
    "        if (key == 0):\n",
    "            max_value = max(value)\n",
    "            argmax = key\n",
    "        else:\n",
    "            tmp = max(value)\n",
    "            if(max_value < tmp):\n",
    "                max_value = tmp\n",
    "                argmax = key\n",
    "    return argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_naive_bayes(x_train, y_train, x_validate, y_validate):\n",
    "    \n",
    "    mean = x_train.groupby(y_train).apply(np.mean)\n",
    "    stdev = x_train.groupby(y_train).apply(np.std)\n",
    "    probabilities = x_train.groupby(y_train).apply(lambda x: len(x) / x_train.shape[0])\n",
    "    y_class = np.unique(y_train)\n",
    "    bayes_model = {\"mean\":mean, \"stdev\":stdev, \"probabilities\":probabilities, \"class\":y_class}\n",
    "    y_predict = predict_naive_bayes(bayes_model, x_validate)\n",
    "    \n",
    "    return y_predict, bayes_model, get_metrics(y_validate, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reg_logistic(model, x_validate):\n",
    "    y_predict = []\n",
    "    for feature in x_validate.values:\n",
    "        value = 0\n",
    "        for i in range(len(feature)):\n",
    "            value += feature[i] * model[i][0]\n",
    "        value_sigmoid = sigmoid(value)\n",
    "        if value_sigmoid >= 0.5:\n",
    "            y_predict.append(1)\n",
    "        else:\n",
    "            y_predict.append(0)\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([2, 1]), name = \"weight\", dtype = tf.float32)\n",
    "bias = tf.Variable(tf.zeros([]), name = \"bias\", dtype = tf.float32)\n",
    "\n",
    "learning_rate = tf.placeholder(shape = [], name = \"learning_rate\", dtype = tf.float32)\n",
    "factor_reg = tf.placeholder(tf.float32)\n",
    "tensor_x = tf.placeholder(shape = [None, 2], name = \"tensor_x\", dtype = tf.float32)\n",
    "tensor_y = tf.placeholder(shape = [None, 1], name = \"tensor_y\", dtype = tf.float32)\n",
    "\n",
    "with tf.name_scope(\"logits\"):\n",
    "    logits = tf.matmul(tensor_x, weight) + bias\n",
    "    \n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    regularization = tf.nn.l2_loss(weight);\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tensor_y)) + (factor_reg*regularization)\n",
    "    cross_entropy_summary = tf.summary.scalar(name=\"cross_entropy\",tensor=cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits,1), tf.argmax(tensor_y,1)), tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar(name=\"accuracy\",tensor=accuracy)\n",
    "\n",
    "with tf.name_scope(\"gradient\"):\n",
    "    gradient = tf.gradients(cross_entropy, weight)\n",
    "\n",
    "with tf.name_scope(\"new_weight\"):\n",
    "    new_weight = tf.assign(weight, weight - learning_rate * gradient[0])\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_reg_logistic(x_train, y_train, x_validate, y_validate, epochs, batch_size, total_iter, lr, rg):\n",
    "    with tf.train.MonitoredSession() as session:\n",
    "        session.run(init)\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(total_iter):\n",
    "                start_index = i*batch_size\n",
    "                end_index = start_index+batch_size\n",
    "                x = np.array(x_train[start_index:end_index])\n",
    "                y = np.array(y_train[start_index:end_index]).reshape(batch_size,1)\n",
    "\n",
    "                feed_dict = {tensor_x:x, tensor_y:y, learning_rate:lr, factor_reg:rg}\n",
    "                _, c, a, w, b= session.run([new_weight, cross_entropy, accuracy, weight, bias],feed_dict=feed_dict)\n",
    "    reg_logistic_model = [w,b]\n",
    "    y_predict = predict_reg_logistic(reg_logistic_model, x_validate)\n",
    "        \n",
    "    return y_predict, reg_logistic_model, get_metrics(y_validate, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentos\n",
    "\n",
    "#### Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Decision Tree | Accuracy: 81.78% | Error: 18.22% | Precisión: 81.55% | Recall: 81.78% | F1: 81.57%\n"
     ]
    }
   ],
   "source": [
    "prediction, tree_model, metrics = model_decision_tree(x_train[used_features], y_train, x_validate[used_features], y_validate)\n",
    "generate_model_x_record(\"decisionTreeLog_varFare_varSex_varClass\", \"Decision Tree\", model, metrics, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: SVM | Accuracy: 80.37% | Error: 19.63% | Precisión: 80.13% | Recall: 80.37% | F1: 80.19%\n"
     ]
    }
   ],
   "source": [
    "prediction, svm_model, metrics = model_svm(x_train[used_features], y_train, x_validate[used_features], y_validate)\n",
    "generate_model_x_record(\"svmLog_varFare_varSex_varClass\", \"SVM\", model, metrics, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Naive Bayes | Accuracy: 76.17% | Error: 23.83% | Precisión: 78.91% | Recall: 76.17% | F1: 74.37%\n"
     ]
    }
   ],
   "source": [
    "prediction, bayes_model, metrics = model_naive_bayes(x_train[used_features], y_train, x_validate[used_features], y_validate)\n",
    "generate_model_x_record(\"naiveBayesLog_varFare_varSex_varClass\", \"Naive Bayes\", model, metrics, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Model: Regression Logistic | Accuracy: 79.91% | Error: 20.09% | Precisión: 79.94% | Recall: 79.91% | F1: 79.61%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 20\n",
    "epochs = 5\n",
    "lr= 0.0001\n",
    "regularization = 0.01\n",
    "sample_size = len(x_train)\n",
    "total_iter = int(sample_size / batch_size)\n",
    "\n",
    "prediction, reg_logistic_model, metrics = model_reg_logistic(x_train.values, y_train.values, x_validate, y_validate, \n",
    "                                                epochs, batch_size, total_iter, lr, regularization)\n",
    "\n",
    "str_config = \"regLogisticLog_lr=\"+str(lr)+\"_reg=\"+str(regularization)+\"_bath_size=\"+str(batch_size)+\"_varFare_varSex_varClass\"\n",
    "generate_model_x_record(str_config, \"Regression Logistic\", model, metrics, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones con el set de pruebas (todos los modelos)\n",
    "\n",
    "*Debido a que el modelo generado con SVM y Regresión Logística se hizo en base a dos variables, se debe eliminar del set de datos la característica \"Fare\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_ = x_test.drop(['Fare'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_tree = tree_model.predict(x_test)\n",
    "y_predict_svm = svm_model.predict(x_test_)\n",
    "y_predict_bayes = predict_naive_bayes(bayes_model, x_test)\n",
    "y_predict_reg_log = predict_reg_logistic(reg_logistic_model.values, x_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combinación de predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_joined = np.stack((y_predict_tree, y_predict_svm, y_predict_bayes, y_predict_reg_log), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obteniendo predicción en función de la moda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_moda_predicted = get_final_prediction(y_predict_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabla de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Bayes</th>\n",
       "      <th>Reg. Log.</th>\n",
       "      <th>Moda Predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Decision Tree  SVM  Bayes  Reg. Log.  Moda Predict\n",
       "0                0    0      0          0             0\n",
       "1                0    0      0          0             0\n",
       "2                0    0      0          0             0\n",
       "3                1    0      0          0             0\n",
       "4                0    0      0          0             0\n",
       "..             ...  ...    ...        ...           ...\n",
       "174              0    0      0          0             0\n",
       "175              0    0      0          0             0\n",
       "176              1    1      1          1             1\n",
       "177              0    0      0          0             0\n",
       "178              1    1      0          1             1\n",
       "\n",
       "[179 rows x 5 columns]"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prediction_summary = np.stack((y_predict_tree, y_predict_svm, y_predict_bayes, y_predict_reg_log, y_moda_predicted), axis=-1)\n",
    "df_predictions = pd.DataFrame(y_prediction_summary, columns = [\"Decision Tree\",\"SVM\",\"Bayes\",\"Reg. Log.\",\"Moda Predict\"])\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cálculo de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_tree = get_metrics(y_test, y_predict_tree)\n",
    "metrics_svm = get_metrics(y_test, y_predict_svm)\n",
    "metrics_bayes = get_metrics(y_test, y_predict_bayes)\n",
    "metrics_reg_log = get_metrics(y_test, y_predict_reg_log)\n",
    "metrics_moda_predicted = get_metrics(y_test, y_moda_predicted)\n",
    "metrics_joined = np.stack((metrics_tree, metrics_svm, metrics_bayes, metrics_reg_log, metrics_moda_predicted), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabla de Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Decision Tree</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Bayes</th>\n",
       "      <th>Reg. Log.</th>\n",
       "      <th>Moda Predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Accuracy</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.715084</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.782123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Error</td>\n",
       "      <td>0.122905</td>\n",
       "      <td>0.212291</td>\n",
       "      <td>0.284916</td>\n",
       "      <td>0.212291</td>\n",
       "      <td>0.217877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Precision</td>\n",
       "      <td>0.879815</td>\n",
       "      <td>0.789249</td>\n",
       "      <td>0.706847</td>\n",
       "      <td>0.789249</td>\n",
       "      <td>0.781436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Recall</td>\n",
       "      <td>0.877095</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.715084</td>\n",
       "      <td>0.787709</td>\n",
       "      <td>0.782123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>F1</td>\n",
       "      <td>0.873918</td>\n",
       "      <td>0.788378</td>\n",
       "      <td>0.706940</td>\n",
       "      <td>0.788378</td>\n",
       "      <td>0.781754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Decision Tree       SVM     Bayes  Reg. Log.  Moda Predict\n",
       "Accuracy        0.877095  0.787709  0.715084   0.787709      0.782123\n",
       "Error           0.122905  0.212291  0.284916   0.212291      0.217877\n",
       "Precision       0.879815  0.789249  0.706847   0.789249      0.781436\n",
       "Recall          0.877095  0.787709  0.715084   0.787709      0.782123\n",
       "F1              0.873918  0.788378  0.706940   0.788378      0.781754"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics = pd.DataFrame(metrics_joined, index = [\"Accuracy\",\"Error\",\"Precision\",\"Recall\",\"F1\"], \n",
    "                          columns = [\"Decision Tree\",\"SVM\",\"Bayes\",\"Reg. Log.\",\"Moda Predict\"])\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusiones\n",
    "\n",
    "- De acuerdo al número de experimentos que se realizaron se observa que uno de los modelos que ofrece mayor exactitud es Decision Tree empleando para ello tres variables durante el entrenamiento (fare, sex, class).\n",
    "- Uno de los modelos que más computación parece requerir es Naive Bayes ya que durante el proceso de entreno el tiempo de respuesta es mucho mayo en comparación al resto.\n",
    "- El modelo de regresión logística requirió bastante experimentación con relación a determinar las variables a utilizar y la definición de los hiper-parametros.\n",
    "- La combinación de los modelos no resultó ser la mejor de las alternativas, ya que aún así el modelo Decision Tree continua siendo el mejor modelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anexos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping\n",
    "\n",
    "Este métodlo permite realizar un remuestreo con el objetivo de evitar el sesgo, la idea principal es obtener una muestra con reemplazo de la muestra original N cantidad de veces, donde N viene siendo el tamaño total de la muestra. Al obtener una muestra del mismo tamaño que la original se consigue un estimador y para el caso, deben lograrse varios estimadores.\n",
    "\n",
    "En este proyecto la manera de implementar Bootstrapping hubiera sido de la siguiente manera:\n",
    "\n",
    "De la población con un tamaño de 891 se obtiene una muestra, para el caso la muestra aleatoria será de 500 y de esta se buscará obtener varios estimadores. El estimador debe poseer el mismo tamaño de la muestra original y se logra realizando el proceso repetitivo de obtener una muestra aleatoria con reemplazo. Para el caso se supondrá que el remuestreo se hará 100 veces, lo que quiere decir que se obtendrán 100 estimadores y para cada uno habrá un estadístico que servirá para determinar con mayor exactitud la predicción de sobrevivientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Folds Cross Validation\n",
    "\n",
    "Generalmente al realizar el proceso de muestreo para la creación de modelos se hace diviendo la muestra en entreno y pruebas, el problema de realizarlo de esta manera es que probablemente el set que se utilice para entrenar el modelo pueda poseer un grupo de registros que no sean lo suficientemente útiles para alcanzar un nivel de exactitud alto. En consecuencia surge una técnica de validación cruzada que busca obtener los mejores resultados.\n",
    "\n",
    "*Validación Cruzada o K-fold Cross Validation* consiste en tomar la muestra original y dividirla en K grupos para realizar el entreno y las pruebas. Este proceso se lleva a cabo de forma iterativa tomando en cuenta que cada K será el número total de iteraciones a ejecutarse, por cada iteración se divide la muestra tomando un grupo de pruebas y el resto para entreno, en la siguiente iteración el grupo que se tomo como prueba ahora será parte del entreno y un grupo distinto ahora será el de prueba, logrando que en toda la muestra todos los grupos en algún momento hayan formando tanto del set de pruebas como de entreno. Este método exige un mayor grado de computación debido a la cantidad de iteraciones y el tamaño de la muestra que se procese.\n",
    "\n",
    "Para el proyecto la implementación podría ser de la siguiente manera:\n",
    "\n",
    "- Se toma el set de datos completo que posee un tamaño igual a 891.\n",
    "- Se define el valor de K en el que se dividirá el set de datos, para el ejemplo se tomará un K=40.\n",
    "- Se crearan 40 grupos de aproximadamente 15 registros.\n",
    "- El algoritmo iniciara recorriendo el primer elemento cual será utilizado como pruebas, quiere decir que los 39 grupos restantes servirán para el entrenamiento.\n",
    "- En la siguiente iteración el grupo que anteriormente sirvió como prueba formará parte del entreno y en su lugar el segundo grupo será ahora de pruebas.\n",
    "- El proceso se repite hasta completar los 40 grupos en que se dividio el set de datos.\n",
    "\n",
    "A continuación en la figura se muestra gráficamente como funcionaría el algoritmo.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/estuardozapeta/estuardozapeta-Statistical-Learning-I-Proyecto/master/k-folds.jpg\">\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
