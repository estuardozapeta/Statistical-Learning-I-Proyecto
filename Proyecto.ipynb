{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Proyecto - Statistical Learning I**\n",
    "\n",
    "### Desarrollo del Proyecto\n",
    "\n",
    "#### Paquetes a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\eezg_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.\"):\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.compat.v1.disable_v2_behavior()\n",
    "    tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>passenger_class</th>\n",
       "      <th>passenger_sex</th>\n",
       "      <th>passenger_survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>886</td>\n",
       "      <td>887</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Middle</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>887</td>\n",
       "      <td>888</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "      <td>Upper</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>888</td>\n",
       "      <td>889</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>Lower</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>889</td>\n",
       "      <td>890</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "      <td>Upper</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>891</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "      <td>Lower</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId                                               Name   Age  \\\n",
       "0              1                            Braund, Mr. Owen Harris  22.0   \n",
       "1              2  Cumings, Mrs. John Bradley (Florence Briggs Th...  38.0   \n",
       "2              3                             Heikkinen, Miss. Laina  26.0   \n",
       "3              4       Futrelle, Mrs. Jacques Heath (Lily May Peel)  35.0   \n",
       "4              5                           Allen, Mr. William Henry  35.0   \n",
       "..           ...                                                ...   ...   \n",
       "886          887                              Montvila, Rev. Juozas  27.0   \n",
       "887          888                       Graham, Miss. Margaret Edith  19.0   \n",
       "888          889           Johnston, Miss. Catherine Helen \"Carrie\"   NaN   \n",
       "889          890                              Behr, Mr. Karl Howell  26.0   \n",
       "890          891                                Dooley, Mr. Patrick  32.0   \n",
       "\n",
       "     SibSp  Parch            Ticket     Fare Cabin Embarked passenger_class  \\\n",
       "0        1      0         A/5 21171   7.2500   NaN        S           Lower   \n",
       "1        1      0          PC 17599  71.2833   C85        C           Upper   \n",
       "2        0      0  STON/O2. 3101282   7.9250   NaN        S           Lower   \n",
       "3        1      0            113803  53.1000  C123        S           Upper   \n",
       "4        0      0            373450   8.0500   NaN        S           Lower   \n",
       "..     ...    ...               ...      ...   ...      ...             ...   \n",
       "886      0      0            211536  13.0000   NaN        S          Middle   \n",
       "887      0      0            112053  30.0000   B42        S           Upper   \n",
       "888      1      2        W./C. 6607  23.4500   NaN        S           Lower   \n",
       "889      0      0            111369  30.0000  C148        C           Upper   \n",
       "890      0      0            370376   7.7500   NaN        Q           Lower   \n",
       "\n",
       "    passenger_sex passenger_survived  \n",
       "0               M                  N  \n",
       "1               F                  Y  \n",
       "2               F                  Y  \n",
       "3               F                  Y  \n",
       "4               M                  N  \n",
       "..            ...                ...  \n",
       "886             M                  N  \n",
       "887             F                  Y  \n",
       "888             F                  N  \n",
       "889             M                  Y  \n",
       "890             M                  N  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_titanic = pd.read_csv('data_titanic_proyecto.csv')\n",
    "data_titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limpieza de datos\n",
    "\n",
    "Convertir valores NaN a cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_titanic = data_titanic.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de Variables\n",
    "\n",
    "Posibles variables predictoras\n",
    "- Age\n",
    "- Fare\n",
    "- passenger_class\n",
    "- passenger_sex\n",
    "\n",
    "Variable a predecir\n",
    "\n",
    "- passenger_survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversión de variables para determinar nivel de correlación\n",
    "\n",
    "A continuación se convierten aquellas variables categóricas a un factor númerico, esta conversión se hace únicamente para determinar el nivel de correlación de las variables independientes con la variable dependiente, pero no es una transformación de encoded para el proceso de entrenamiento.\n",
    "\n",
    "*La variable Age no requiere conversión ya que por defecto su valor es númerico*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_titanic['passenger_survived_codes'] = data_titanic['passenger_survived'].astype('category').cat.codes\n",
    "data_titanic['passenger_sex_codes'] = data_titanic['passenger_sex'].astype('category').cat.codes\n",
    "data_titanic['passenger_class_codes'] = data_titanic['passenger_class'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlación de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Age                         0.010539\n",
       "SibSp                      -0.035322\n",
       "Parch                       0.081629\n",
       "Fare                        0.257307\n",
       "passenger_survived_codes    1.000000\n",
       "passenger_sex_codes        -0.543351\n",
       "passenger_class_codes       0.338481\n",
       "Name: passenger_survived_codes, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_titanic[data_titanic.columns[1:]].corr()['passenger_survived_codes'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depuración de features\n",
    "\n",
    "Se eliminan aquellas características que son identificadores, nombres o etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data_titanic.drop(['passenger_survived_codes','PassengerId', 'Name', 'Ticket', 'Cabin', 'Embarked', 'passenger_class', 'passenger_sex', 'passenger_survived'], axis=1)\n",
    "Y=data_titanic['passenger_survived_codes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de mejores features\n",
    "\n",
    "Utilizando la libreria \"SelectKBest\" se determinan las 3 mejores características y sobre esas se trabajan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Fare', 'passenger_sex_codes', 'passenger_class_codes'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "best=SelectKBest(k=3)\n",
    "best.fit_transform(X, Y)\n",
    "selected = best.get_support(indices=True)\n",
    "print(X.columns[selected])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se utilizarán las variables:\n",
    "\n",
    "- Fare\n",
    "- passenger_sex\n",
    "- passenger_class\n",
    "\n",
    "Ya que poseen un alto nivel de correlación y según la libreria \"SelectKBest\" las sugiere como mejores features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_features = X.columns[selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la posibles features \"X\", se eliminan aquellas que no formaran parte del proceso de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(['Age', 'SibSp', 'Parch'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### División de datos para entreno, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Codificación de variables categoricas **\"X\"** y **\"Y\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_passenger_class = pd.get_dummies(data_titanic[\"passenger_sex\"])\n",
    "x = one_hot_passenger_class.to_numpy()\n",
    "\n",
    "one_hot_passenger_survived = pd.get_dummies(data_titanic[\"passenger_survived\"])\n",
    "y = one_hot_passenger_survived.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partición de datos para entreno, validación y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable(s) \"x\" \n",
    "len_train_validate = int(len(x)*0.8)\n",
    "x_train_validate = x[0:len_train_validate]\n",
    "x_test = x[len_train_validate:]\n",
    "\n",
    "len_train = int(len(x_train_validate)*0.7)\n",
    "x_train = x_train_validate[0:len_train]\n",
    "x_validate = x_train_validate[len_train:]\n",
    "\n",
    "# Variable \"y\"\n",
    "len_train_validate = int(len(y)*0.8)\n",
    "y_train_validate = y[0:len_train_validate]\n",
    "y_test = y[len_train_validate:]\n",
    "\n",
    "len_train = int(len(y_train_validate)*0.7)\n",
    "y_train = y_train_validate[0:len_train]\n",
    "y_validate = y_train_validate[len_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Métodos de decodificación para variables categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(encode, dict_label_code):\n",
    "    label_decode = []\n",
    "    for i in range(len(encode)):\n",
    "        label = [key  for (key, value) in dict_label_code.items() if (value == encode[i]).all()]\n",
    "        label_decode = label_decode + label    \n",
    "    return (label_decode)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_x_code(data, label, encode, length):\n",
    "    label_code_append = np.column_stack([data[label].to_numpy(), encode])\n",
    "    label_code_unique = pd.DataFrame(label_code_append).drop_duplicates().to_numpy()\n",
    "    keys = label_code_unique[:,0]\n",
    "    values = label_code_unique[:,1:length+1]\n",
    "    dict_label_code = dict(zip(keys, zip(*values)))\n",
    "    return dict_label_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Función para generación de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_predict):\n",
    "    accuracy = accuracy_score(y_true, y_predict)\n",
    "    error = mean_squared_error(y_true, y_predict)\n",
    "    precision = precision_score(y_true, y_predict, average='weighted')\n",
    "    recall = recall_score(y_true, y_predict, average='weighted')\n",
    "    f1 = f1_score(y_true, y_predict, average=\"weighted\")\n",
    "    \n",
    "    return accuracy, error, precision, recall, f1    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo - Árbol de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_decision_tree(x_train, y_train, x_validate, y_validate):\n",
    "    \n",
    "    tree_model = tree.DecisionTreeClassifier()\n",
    "    tree_model = tree_model.fit(x_train, y_train)\n",
    "    y_predict = tree_model.predict(x_validate)\n",
    "    \n",
    "    return y_predict, tree_model, get_metrics(y_validate, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo - SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_svm(x_train, y_train, x_validate, y_validate):\n",
    "\n",
    "    svm_model = svm.SVC()\n",
    "    svm_model = svm_model.fit(x_train, y_train)\n",
    "    y_predict = svm_model.predict(x_validate)\n",
    "    \n",
    "    return y_predict, svm_model, get_metrics(y_validate, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_naive_bayes(model, x_validate):\n",
    "    y_predict = []\n",
    "    for i in range(x_validate.shape[0]):\n",
    "        probability={}\n",
    "        for y_class in model[3]:\n",
    "            probability[y_class] = model[2].iloc[y_class]\n",
    "            for index, _ in enumerate(x_validate.iloc[i]):\n",
    "                probability[y_class] *= norm.pdf(x_validate.iloc[i], model[0].iloc[y_class, index], model[1].iloc[y_class, index])\n",
    "        y_predict.append(get_argmax(probability))\n",
    "    return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_argmax(probability):\n",
    "    max_value = 0\n",
    "    argmax = -1\n",
    "    for (key, value) in probability.items():\n",
    "        if (key == 0):\n",
    "            max_value = max(value)\n",
    "            argmax = key\n",
    "        else:\n",
    "            tmp = max(value)\n",
    "            if(max_value < tmp):\n",
    "                max_value = tmp\n",
    "                argmax = key\n",
    "    return argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_naive_bayes(x_train, y_train, x_validate, y_validate):\n",
    "    \n",
    "    mean = x_train.groupby(y_train).apply(np.mean)\n",
    "    stdev = x_train.groupby(y_train).apply(np.std)\n",
    "    probabilities = x_train.groupby(y_train).apply(lambda x: len(x) / x_train.shape[0])\n",
    "    y_class = np.unique(y_train)\n",
    "    bayes_model = [mean, stdev, probabilities, y_class]    \n",
    "    y_predict = predict_naive_bayes(bayes_model, x_validate)\n",
    "        \n",
    "    return y_predict, bayes_model, get_metrics(y_validate, y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo - Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial_features = PolynomialFeatures(2)\n",
    "array_x = x_train.values #polynomial_features.fit_transform(x_train.values)\n",
    "array_y = y_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definición del Grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\eezg_\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py:201: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([3, 1]), name = \"weight\", dtype = tf.float32)\n",
    "bias = tf.Variable(tf.zeros([]), name = \"bias\", dtype = tf.float32)\n",
    "\n",
    "learning_rate = tf.placeholder(shape = [], name = \"learning_rate\", dtype = tf.float32)\n",
    "regularization_factor = tf.placeholder(tf.float32)\n",
    "tensor_x = tf.placeholder(shape = [None, 3], name = \"tensor_x\", dtype = tf.float32)\n",
    "tensor_y = tf.placeholder(shape = [None, 1], name = \"tensor_y\", dtype = tf.float32)\n",
    "\n",
    "with tf.name_scope(\"logits\"):\n",
    "    logits = tf.matmul(tensor_x, weight) + bias\n",
    "    \n",
    "with tf.name_scope(\"cross_entropy\"):\n",
    "    regularization = tf.nn.l2_loss(weight);\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = tensor_y)) + (regularization_factor*regularization)\n",
    "    cross_entropy_summary = tf.summary.scalar(name=\"cross_entropy\",tensor=cross_entropy)\n",
    "\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits,1), tf.argmax(tensor_y,1)), tf.float32))\n",
    "    accuracy_summary = tf.summary.scalar(name=\"accuracy\",tensor=accuracy)\n",
    "\n",
    "with tf.name_scope(\"gradient\"):\n",
    "    gradient = tf.gradients(cross_entropy, weight)\n",
    "\n",
    "with tf.name_scope(\"new_weight\"):\n",
    "    new_weight = tf.assign(weight, weight - learning_rate * gradient[0])\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8912509381337456, 0.9120108393559098, 0.933254300796991, 0.954992586021436]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regularizators = [pow(10,i) for i in np.arange(-0.05, -0.01, 0.01)]\n",
    "regularizators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "sample_size = len(x_train)\n",
    "total_iterations = int(sample_size / batch_size)\n",
    "\n",
    "def train(epochs, lr):\n",
    "    with tf.train.MonitoredSession() as session:\n",
    "        session.run(init)\n",
    "        #writer = tf.summary.FileWriter(\"./graphs/\"+datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\"_lr=\"+str(lr), session.graph)\n",
    "        for regul_factor in regularizators:\n",
    "            for epoch in range(epochs):\n",
    "                for i in range(total_iterations):\n",
    "                    start_index = i*batch_size\n",
    "                    end_index = start_index+batch_size\n",
    "                    x = np.array(array_x[start_index:end_index])\n",
    "                    y = np.array(array_y[start_index:end_index]).reshape(20,1)\n",
    "\n",
    "                    feed_dict = {tensor_x:x, tensor_y:y, learning_rate:lr, regularization_factor:regul_factor}\n",
    "                    entropy_summary = session.run(cross_entropy_summary, feed_dict=feed_dict)\n",
    "                    acc_summary = session.run(accuracy_summary, feed_dict=feed_dict)\n",
    "\n",
    "                    _, c, a, w, b= session.run([new_weight, cross_entropy, accuracy, weight, bias],feed_dict=feed_dict)\n",
    "                    if (i % 20 == 0):\n",
    "                        print(\"Epoch: {} Iteration: {} Cross Entropy: {} Accuracy: {}\".format(epoch,i,c,a))               \n",
    "                        #writer.add_summary(entropy_summary, i)\n",
    "                        #writer.add_summary(acc_summary, i)\n",
    "            #writer.close()\n",
    "    return (w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Epoch: 0 Iteration: 0 Cross Entropy: 0.8877283334732056 Accuracy: 1.0\n",
      "Epoch: 0 Iteration: 20 Cross Entropy: 0.8901100158691406 Accuracy: 1.0\n",
      "Epoch: 1 Iteration: 0 Cross Entropy: 0.8906721472740173 Accuracy: 1.0\n",
      "Epoch: 1 Iteration: 20 Cross Entropy: 0.8930597305297852 Accuracy: 1.0\n",
      "Epoch: 2 Iteration: 0 Cross Entropy: 0.8936232328414917 Accuracy: 1.0\n",
      "Epoch: 2 Iteration: 20 Cross Entropy: 0.8960162401199341 Accuracy: 1.0\n",
      "Epoch: 3 Iteration: 0 Cross Entropy: 0.8965811133384705 Accuracy: 1.0\n",
      "Epoch: 3 Iteration: 20 Cross Entropy: 0.8989799618721008 Accuracy: 1.0\n",
      "Epoch: 4 Iteration: 0 Cross Entropy: 0.8995461463928223 Accuracy: 1.0\n",
      "Epoch: 4 Iteration: 20 Cross Entropy: 0.9019507765769958 Accuracy: 1.0\n",
      "Epoch: 5 Iteration: 0 Cross Entropy: 0.9025182127952576 Accuracy: 1.0\n",
      "Epoch: 5 Iteration: 20 Cross Entropy: 0.9049287438392639 Accuracy: 1.0\n",
      "Epoch: 6 Iteration: 0 Cross Entropy: 0.905497670173645 Accuracy: 1.0\n",
      "Epoch: 6 Iteration: 20 Cross Entropy: 0.9079139828681946 Accuracy: 1.0\n",
      "Epoch: 7 Iteration: 0 Cross Entropy: 0.9084843397140503 Accuracy: 1.0\n",
      "Epoch: 7 Iteration: 20 Cross Entropy: 0.910905659198761 Accuracy: 1.0\n",
      "Epoch: 8 Iteration: 0 Cross Entropy: 0.9114773869514465 Accuracy: 1.0\n",
      "Epoch: 8 Iteration: 20 Cross Entropy: 0.9139047861099243 Accuracy: 1.0\n",
      "Epoch: 9 Iteration: 0 Cross Entropy: 0.9144778251647949 Accuracy: 1.0\n",
      "Epoch: 9 Iteration: 20 Cross Entropy: 0.9169110655784607 Accuracy: 1.0\n",
      "Epoch: 0 Iteration: 0 Cross Entropy: 0.9388564825057983 Accuracy: 1.0\n",
      "Epoch: 0 Iteration: 20 Cross Entropy: 0.9413446187973022 Accuracy: 1.0\n",
      "Epoch: 1 Iteration: 0 Cross Entropy: 0.9419321417808533 Accuracy: 1.0\n",
      "Epoch: 1 Iteration: 20 Cross Entropy: 0.944425642490387 Accuracy: 1.0\n",
      "Epoch: 2 Iteration: 0 Cross Entropy: 0.9450147151947021 Accuracy: 1.0\n",
      "Epoch: 2 Iteration: 20 Cross Entropy: 0.9475139379501343 Accuracy: 1.0\n",
      "Epoch: 3 Iteration: 0 Cross Entropy: 0.948104202747345 Accuracy: 1.0\n",
      "Epoch: 3 Iteration: 20 Cross Entropy: 0.950609564781189 Accuracy: 1.0\n",
      "Epoch: 4 Iteration: 0 Cross Entropy: 0.9512010812759399 Accuracy: 1.0\n",
      "Epoch: 4 Iteration: 20 Cross Entropy: 0.9537121057510376 Accuracy: 1.0\n",
      "Epoch: 5 Iteration: 0 Cross Entropy: 0.9543051719665527 Accuracy: 1.0\n",
      "Epoch: 5 Iteration: 20 Cross Entropy: 0.9568221569061279 Accuracy: 1.0\n",
      "Epoch: 6 Iteration: 0 Cross Entropy: 0.9574166536331177 Accuracy: 1.0\n",
      "Epoch: 6 Iteration: 20 Cross Entropy: 0.9599388241767883 Accuracy: 1.0\n",
      "Epoch: 7 Iteration: 0 Cross Entropy: 0.9605346322059631 Accuracy: 1.0\n",
      "Epoch: 7 Iteration: 20 Cross Entropy: 0.9630628824234009 Accuracy: 1.0\n",
      "Epoch: 8 Iteration: 0 Cross Entropy: 0.9636601209640503 Accuracy: 1.0\n",
      "Epoch: 8 Iteration: 20 Cross Entropy: 0.9661942720413208 Accuracy: 1.0\n",
      "Epoch: 9 Iteration: 0 Cross Entropy: 0.9667928814888 Accuracy: 1.0\n",
      "Epoch: 9 Iteration: 20 Cross Entropy: 0.9693328142166138 Accuracy: 1.0\n",
      "Epoch: 0 Iteration: 0 Cross Entropy: 0.9925254583358765 Accuracy: 1.0\n",
      "Epoch: 0 Iteration: 20 Cross Entropy: 0.995121419429779 Accuracy: 1.0\n",
      "Epoch: 1 Iteration: 0 Cross Entropy: 0.9957351684570312 Accuracy: 1.0\n",
      "Epoch: 1 Iteration: 20 Cross Entropy: 0.9983372688293457 Accuracy: 1.0\n",
      "Epoch: 2 Iteration: 0 Cross Entropy: 0.9989522695541382 Accuracy: 1.0\n",
      "Epoch: 2 Iteration: 20 Cross Entropy: 1.0015605688095093 Accuracy: 1.0\n",
      "Epoch: 3 Iteration: 0 Cross Entropy: 1.0021770000457764 Accuracy: 1.0\n",
      "Epoch: 3 Iteration: 20 Cross Entropy: 1.004791259765625 Accuracy: 1.0\n",
      "Epoch: 4 Iteration: 0 Cross Entropy: 1.0054090023040771 Accuracy: 1.0\n",
      "Epoch: 4 Iteration: 20 Cross Entropy: 1.008028507232666 Accuracy: 1.0\n",
      "Epoch: 5 Iteration: 0 Cross Entropy: 1.0086479187011719 Accuracy: 1.0\n",
      "Epoch: 5 Iteration: 20 Cross Entropy: 1.0112733840942383 Accuracy: 1.0\n",
      "Epoch: 6 Iteration: 0 Cross Entropy: 1.0118939876556396 Accuracy: 1.0\n",
      "Epoch: 6 Iteration: 20 Cross Entropy: 1.0145255327224731 Accuracy: 1.0\n",
      "Epoch: 7 Iteration: 0 Cross Entropy: 1.0151474475860596 Accuracy: 1.0\n",
      "Epoch: 7 Iteration: 20 Cross Entropy: 1.0177849531173706 Accuracy: 1.0\n",
      "Epoch: 8 Iteration: 0 Cross Entropy: 1.0184084177017212 Accuracy: 1.0\n",
      "Epoch: 8 Iteration: 20 Cross Entropy: 1.0210514068603516 Accuracy: 1.0\n",
      "Epoch: 9 Iteration: 0 Cross Entropy: 1.0216763019561768 Accuracy: 1.0\n",
      "Epoch: 9 Iteration: 20 Cross Entropy: 1.024324893951416 Accuracy: 1.0\n",
      "Epoch: 0 Iteration: 0 Cross Entropy: 1.0488251447677612 Accuracy: 1.0\n",
      "Epoch: 0 Iteration: 20 Cross Entropy: 1.0515328645706177 Accuracy: 1.0\n",
      "Epoch: 1 Iteration: 0 Cross Entropy: 1.052173137664795 Accuracy: 1.0\n",
      "Epoch: 1 Iteration: 20 Cross Entropy: 1.0548866987228394 Accuracy: 1.0\n",
      "Epoch: 2 Iteration: 0 Cross Entropy: 1.0555285215377808 Accuracy: 1.0\n",
      "Epoch: 2 Iteration: 20 Cross Entropy: 1.0582473278045654 Accuracy: 1.0\n",
      "Epoch: 3 Iteration: 0 Cross Entropy: 1.058890461921692 Accuracy: 1.0\n",
      "Epoch: 3 Iteration: 20 Cross Entropy: 1.0616153478622437 Accuracy: 1.0\n",
      "Epoch: 4 Iteration: 0 Cross Entropy: 1.0622599124908447 Accuracy: 1.0\n",
      "Epoch: 4 Iteration: 20 Cross Entropy: 1.0649911165237427 Accuracy: 1.0\n",
      "Epoch: 5 Iteration: 0 Cross Entropy: 1.0656371116638184 Accuracy: 1.0\n",
      "Epoch: 5 Iteration: 20 Cross Entropy: 1.0683742761611938 Accuracy: 1.0\n",
      "Epoch: 6 Iteration: 0 Cross Entropy: 1.0690215826034546 Accuracy: 1.0\n",
      "Epoch: 6 Iteration: 20 Cross Entropy: 1.0717641115188599 Accuracy: 1.0\n",
      "Epoch: 7 Iteration: 0 Cross Entropy: 1.0724129676818848 Accuracy: 1.0\n",
      "Epoch: 7 Iteration: 20 Cross Entropy: 1.0751614570617676 Accuracy: 1.0\n",
      "Epoch: 8 Iteration: 0 Cross Entropy: 1.075811743736267 Accuracy: 1.0\n",
      "Epoch: 8 Iteration: 20 Cross Entropy: 1.078566074371338 Accuracy: 1.0\n",
      "Epoch: 9 Iteration: 0 Cross Entropy: 1.079217553138733 Accuracy: 1.0\n",
      "Epoch: 9 Iteration: 20 Cross Entropy: 1.08197820186615 Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "w,b=train(10, 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2494775 ],\n",
       "       [-0.79637516],\n",
       "       [-0.268143  ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0008193803830555108"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-7.1061424175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_reg_logistic(x_validate, weight):\n",
    "    y_predict = []\n",
    "    for feature in x_validate.values:\n",
    "        value = 0\n",
    "        for i in range(len(feature)):\n",
    "            value += feature[i] * weight[i][0]\n",
    "        value_sigmoid = sigmoid(value)\n",
    "        if value_sigmoid >= 0.5:\n",
    "            y_predict.append(1)\n",
    "        else:\n",
    "            y_predict.append(0)\n",
    "    return y_predict        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = predict_reg_logistic(x_validate, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6308411214953271"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_validate, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, model, metrics = model_decision_tree(x_train[used_features], y_train, x_validate[used_features], y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, model, metrics = model_svm(x_train[used_features], y_train, x_validate[used_features], y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction, model, metrics = model_naive_bayes(x_train[used_features], y_train, x_validate[used_features], y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bootstrapping\n",
    "\n",
    "Este métodlo permite realizar un remuestreo con el objetivo de evitar el sesgo, la idea principal es obtener una muestra con reemplazo de la muestra original N cantidad de veces, donde N viene siendo el tamaño total de la muestra. Al obtener una muestra del mismo tamaño que la original se consigue un estimador y para el caso, deben lograrse varios estimadores.\n",
    "\n",
    "En este proyecto la manera de implementar Bootstrapping hubiera sido de la siguiente manera:\n",
    "\n",
    "De la población con un tamaño de 891 se obtiene una muestra, para el caso la muestra aleatoria será de 500 y de esta se buscará obtener varios estimadores. El estimador debe poseer el mismo tamaño de la muestra original y se logra realizando el proceso repetitivo de obtener una muestra aleatoria con reemplazo. Para el caso se supondrá que el remuestreo se hará 100 veces, lo que quiere decir que se obtendrán 100 estimadores y para cada uno habrá un estadístico que servirá para determinar con mayor exactitud la predicción de sobrevivientes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Folds Cross Validation\n",
    "\n",
    "Esta técnica \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decodificando predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_x_code = get_label_x_code(data_validate, \"passenger_survived\", y_validate, 2)\n",
    "label_decoded = decode(tree_predict, label_x_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_x_code = np.column_stack([data_train['passenger_survived'].to_numpy(), passenger_survived_encoded])\n",
    "df = pd.DataFrame(np.column_stack([data_train['passenger_survived'].to_numpy(), passenger_survived_encoded]))\n",
    "survived = df.drop_duplicates().to_numpy()\n",
    "keys = survived[:,0]\n",
    "values = survived[:,1:3]\n",
    "dict_survived = dict(zip(keys, zip(*values)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
